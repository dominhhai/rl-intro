{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evaluation (Prediction)\n",
    "- Iterative Policy Evaluation:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= E_{\\pi}[R_{t+1}+\\gamma v_k(S_{t+1}) | S_t=s]\n",
    "\\\\ &= \\sum_a\\pi(a | s)\\sum_{s^\\prime, r} p(s^\\prime, r | s,a)\\big[ r+\\gamma v_k(s^\\prime) \\big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Initial $v_0$ is chosen arbitrarily, however, all terminal state must be 0\n",
    "- $k\\to\\infty$, sequence $\\{v_k\\}$ can be converge to $v_\\pi$\n",
    "- Each update step is **expected update**\n",
    "\n",
    "![Iterative Policy Evaluation](assets/4.1.algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Improvement\n",
    "- Policy improvement:\n",
    "    - if $q_\\pi(s, \\pi^\\prime(s))\\ge v_\\pi(s)$, then $v_{\\pi^\\prime}(s)\\ge v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policies and Value Functions\n",
    "- Policy: mapping from states to probabilities of selecting each possible action\n",
    "$$\\pi(a | s) = Pr(A_t = a | S_t = s)$$\n",
    "- State-Value function for policy $\\pi$\n",
    "$$v_{\\pi}(s) = E_{\\pi}[G_t | S_t=s]=E_{\\pi}\\Big[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1} | S_t=s\\Big], ~~~\\forall s\\in\\mathcal S$$\n",
    "- Action-Value funciton for policy $\\pi$\n",
    "$$q_{\\pi}(s, a) = E_{\\pi}[G_t | S_t=s, A_t=a]=E_{\\pi}\\Big[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1} | S_t=s, A_t=a\\Big]$$\n",
    "- Bellman equation for $v_\\pi$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_\\pi(s) &= E_{\\pi}[G_t | S_t=s]\n",
    "\\\\ &= E_{\\pi}[R_{t+1}+\\gamma G_{t+1} | S_t=s]\n",
    "\\\\ &= \\sum_a\\pi(a | s)\\sum_{s^\\prime}\\sum_r p(s^\\prime, r | s,a)\\Big[ r+\\gamma E_\\pi(G_{t+1} | S_{t+1}=s^\\prime) \\Big]\n",
    "\\\\ &= \\sum_a\\pi(a | s)\\sum_{s^\\prime, r} p(s^\\prime, r | s,a)\\Big[ r+\\gamma v_\\pi(s^\\prime) \\Big]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Optimal Policies and Optimal Value Functions\n",
    "- find a policy that achieves a lot of reward over the long run\n",
    "- $\\pi$ is better than $\\pi^\\prime$: $\\pi \\ge \\pi^\\prime \\text{ iff } v_\\pi(s) \\ge v_{\\pi^\\prime}(s), ~~~\\forall s\\in\\mathcal S$\n",
    "- Optimal Policy: $$\\pi_*=\\max(\\pi), ~~~\\forall \\pi$$\n",
    "- Optimal state-value function: $$v_*(s) = \\max\\limits_\\pi v_\\pi(s), ~~~\\forall s\\in\\mathcal S$$\n",
    "- Optimal action-value funciton: $$q_*(s) = \\max\\limits_\\pi v_\\pi(s, a)=E\\Big[(R_{t+1}+\\gamma v_*(S_{t+1}) | S_t=s,A_t=a\\Big], ~~~\\forall s\\in\\mathcal S, a\\in\\mathcal A(s)$$\n",
    "\n",
    "- Hard, extensive to learn optimal policy, so, can only approximate to varying degress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
