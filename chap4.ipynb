{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agent-Environment Interface\n",
    "![agent-enviroment](assets/agent-env.png)\n",
    "\n",
    "- Interact at each of sequence of discrete time steps, $t=0,1,2,3,...$\n",
    "- Each step $t$\n",
    "    - agent receives enviroment's **state** $S_t \\in \\mathcal S$\n",
    "    - agent selects **action** $A_t \\in \\mathcal A(s)$\n",
    "- After that\n",
    "    - enviroment emits **reward** $R_{t+1}\\in \\mathcal R \\in \\mathbb R$\n",
    "    - enviroment emits **state** $S_{t+1}\\in \\mathcal S$\n",
    "- Trajectory (history) $S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,...$\n",
    "- Finite MDP: $|\\mathcal S|, |\\mathcal A|, |\\mathcal R|$ are finite\n",
    "- **Dynamics of MDP**:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(s^{\\prime}, r | s,a) &= Pr\\{S_t=s^{\\prime}, R_t=r | S_{t-1}=s, A_{t-1}=a\\}\n",
    "\\\\\n",
    "p(s^{\\prime} | s,a) &= Pr\\{S_t=s^{\\prime} | S_{t-1}=s, A_{t-1}=a\\} = \\sum_{r\\in\\mathcal R} p(s^{\\prime}, r | s,a)\n",
    "\\\\\n",
    "r(s,a) &= E\\{R_{t+1} | S_t=s, A_t=a\\} = \\sum_{r\\in\\mathcal R} r \\sum_{s^{\\prime}\\in\\mathcal S} p(s^{\\prime}, r | s,a)\n",
    "\\end{aligned}\n",
    "$$\n",
    "- **Markov Property**: Probability of each possible value for $S_t, R_t$ depends only on $S_{t-1}, A_{t-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Goals and Returns\n",
    "- **Goal**: Maximize the expected value of the cumulative reward in the long run\n",
    "- Maximize the *expected discounted return*:\n",
    "$$G_t=R_{t+1}+\\gamma R_{t+2}+\\gamma^2 R_{t+3}+...=\\sum_{k=t+1}^T \\gamma^{k-t-1}R_k=R_{t+1}+\\gamma G_{t+1}$$\n",
    "  where $0\\le\\gamma < 1$ is the *discount rate*\n",
    "  - Possiblity that, $T=\\infty \\text{ or } \\gamma=1$, but not both\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policies and Value Functions\n",
    "- Policy: mapping from states to probabilities of selecting each possible action\n",
    "$$\\pi(a | s) = Pr(A_t = a | S_t = s)$$\n",
    "- State-Value function for policy $\\pi$\n",
    "$$v_{\\pi}(s) = E_{\\pi}[G_t | S_t=s]=E_{\\pi}\\Big[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1} | S_t=s\\Big], ~~~\\forall s\\in\\mathcal S$$\n",
    "- Action-Value funciton for policy $\\pi$\n",
    "$$q_{\\pi}(s, a) = E_{\\pi}[G_t | S_t=s, A_t=a]=E_{\\pi}\\Big[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1} | S_t=s, A_t=a\\Big]$$\n",
    "- Bellman equation for $v_\\pi$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_\\pi(s) &= E_{\\pi}[G_t | S_t=s]\n",
    "\\\\ &= E_{\\pi}[R_{t+1}+\\gamma G_{t+1} | S_t=s]\n",
    "\\\\ &= \\sum_a\\pi(a | s)\\sum_{s^\\prime}\\sum_r p(s^\\prime, r | s,a)\\Big[ r+\\gamma E_\\pi(G_{t+1} | S_{t+1}=s^\\prime) \\Big]\n",
    "\\\\ &= \\sum_a\\pi(a | s)\\sum_{s^\\prime, r} p(s^\\prime, r | s,a)\\Big[ r+\\gamma v_\\pi(s^\\prime) \\Big]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Optimal Policies and Optimal Value Functions\n",
    "- find a policy that achieves a lot of reward over the long run\n",
    "- $\\pi$ is better than $\\pi^\\prime$: $\\pi \\ge \\pi^\\prime \\text{ iff } v_\\pi(s) \\ge v_{\\pi^\\prime}(s), ~~~\\forall s\\in\\mathcal S$\n",
    "- Optimal Policy: $$\\pi_*=\\max(\\pi), ~~~\\forall \\pi$$\n",
    "- Optimal state-value function: $$v_*(s) = \\max\\limits_\\pi v_\\pi(s), ~~~\\forall s\\in\\mathcal S$$\n",
    "- Optimal action-value funciton: $$q_*(s) = \\max\\limits_\\pi v_\\pi(s, a)=E\\Big[(R_{t+1}+\\gamma v_*(S_{t+1}) | S_t=s,A_t=a\\Big], ~~~\\forall s\\in\\mathcal S, a\\in\\mathcal A(s)$$\n",
    "\n",
    "- Hard, extensive to learn optimal policy, so, can only approximate to varying degress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
