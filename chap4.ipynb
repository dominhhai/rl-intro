{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evaluation (Prediction)\n",
    "- Given policy $\\pi$, compute the state-value function $v_\\pi$\n",
    "- Iterative Policy Evaluation $\\forall s\\in\\mathcal S$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{k+1}(s) &= E_{\\pi}[R_{t+1}+\\gamma v_k(S_{t+1}) | S_t=s]\n",
    "\\\\ &= \\sum_a\\pi(a | s)\\sum_{s^\\prime, r} p(s^\\prime, r | s,a)\\big[ r+\\gamma v_k(s^\\prime) \\big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Initial $v_0$ is chosen arbitrarily, however, all terminal state must be 0\n",
    "- $k\\to\\infty$, sequence $\\{v_k\\}$ can be converge to $v_\\pi$\n",
    "- Each update step is **expected update**\n",
    "\n",
    "![Iterative Policy Evaluation](assets/4.1.algo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy Improvement\n",
    "- Policy improvement:\n",
    "    - if $q_\\pi(s, \\pi^\\prime(s))\\ge v_\\pi(s)$, then $v_{\\pi^\\prime}(s)\\ge v_\\pi(s)$\n",
    "    - where, $\\displaystyle q_\\pi(s,a)=E\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1}) | S_t=s,A_t=a\\big]=\\sum_{s^\\prime,r}p(s^\\prime,r | s,a)[r+\\gamma v_\\pi(s^\\prime)]$\n",
    " \n",
    "- new **greedy policy**, $\\pi'$ takes the action that looks best in the short term -- after one step of lookahead according to $v_\\pi$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\pi'(s) &= \\arg\\max\\limits_a q_\\pi(s,a)\n",
    "\\\\ &= \\arg\\max\\limits_a E\\big[R_{t+1}+\\gamma v_\\pi(S_{t+1}) | S_t=s,A_t=a\\big] \n",
    "\\\\ &= \\arg\\max\\limits_a \\sum_{s^\\prime,r}p(s^\\prime,r | s,a)[r+\\gamma v_\\pi(s^\\prime)]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- if $v_{\\pi'}=v_\\pi$, then $v_{\\pi'}=v_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Policy Iteration\n",
    "- Generalized Policy Iteration\n",
    "![General Policy Iteration](assets/4.6.policy_iteration.png)\n",
    "\n",
    "- Policy Iteration Algorithm:\n",
    "    - On policy $\\pi$, using $v_\\pi$ to yield a better policy $\\pi'$\n",
    "    - With $\\pi'$, computing $v_{\\pi'}$ to impove it a again to yield an even better $\\pi''$\n",
    "    \n",
    "![Policy Iteration](assets/4.2.policy_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Value Iteration\n",
    "- Value Iteration, $\\forall s\\in\\mathcal S$:\n",
    "$$v_{k+1}(s)=\\max_a\\sum_{s',r}p(s',r | s,a)\\big[r+\\gamma v_k(s')\\big]$$\n",
    "    - compute for optimal policy only\n",
    "    - truncated policy iteration\n",
    "    - policy evaluation is stopped after just one sweep\n",
    "\n",
    "![Value Iteration](assets/4.3.value_iteration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "- Policy evaluation: backups without a max\n",
    "- Policy improvement: form a greedy policy, if only locally\n",
    "- Policy iteration: alternate the above two processes\n",
    "- Value iteration: backups with a max\n",
    "- Full backups (to be contrasted later with sample backups)\n",
    "- Generalized Policy Iteration (GPI)\n",
    "- Asynchronous DP: a way to avoid exhausitve sweeps by random selecting a state to compute at each step\n",
    "- Bootstrapping: update estimates based on other estimates\n",
    "- Biggest limiation of DP is that it requires a *probability model* (as opposed to a generative or simulation model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
