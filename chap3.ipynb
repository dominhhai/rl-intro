{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Finite Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Agent-Environment Interface\n",
    "![agent-enviroment](assets/agent-env.png)\n",
    "\n",
    "- Interact at each of sequence of discrete time steps, $t=0,1,2,3,...$\n",
    "- Each step $t$, agent receives enviroment's **state** $S_t \\in \\mathcal S$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration vs Exploitation\n",
    "- Greedy Action at time $t$ is $A_t^* =\\arg\\max\\limits_a Q_t(a)$\n",
    "- *Exploiting* if $A_t = A_t^*$\n",
    "- *Exploring* if $A_t \\neq A_t^*$\n",
    "- Exploitaion maximizes the expected reward on the one step\n",
    "- Exploration may produce the greater total reward in the long run\n",
    "- Can't do both with any single action selection\n",
    "- Need to balance Exploitation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action-value Methods\n",
    "- Estimate the values of actions and use the estimates to make action selection decisions\n",
    "- *sample-average* method:\n",
    "$$Q_t(a)=\\dfrac{\\sum_{i=1}^{t-1}R_i \\cdot \\mathbb 1_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathbb 1_{A_i=a}}$$\n",
    "- $Q_t(a)$ coverages to $q_*(a)$ by the law of large numbers :\n",
    "$$\\lim\\limits_{N_t(a)\\rightarrow\\infty}Q_t(a)=q_*(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ε-greedy Methods\n",
    "- Usually select greedy actions\n",
    "- Random pick an action (includes non-greedy actions) with probability `ε`\n",
    "- Every action may be selected, all the $Q_t(a)$ can coverage to $q_*(a)$\n",
    "- Possible to reduce `ε` over time to try to get the best of both high and low values\n",
    "\n",
    "***************************************\n",
    "Initialize, for $a = 1$ to $k$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(a) & \\leftarrow 0\n",
    "\\\\\n",
    "N(a) & \\leftarrow 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Loop forever:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A & \\leftarrow\n",
    "    \\begin{cases}\n",
    "        \\arg\\max_a Q(a) &\\text{with probability }(1-\\epsilon)\n",
    "        \\\\\n",
    "        \\text{a random action} &\\text{with probability }\\epsilon\n",
    "    \\end{cases}\n",
    "\\\\\n",
    "R & \\leftarrow \\text{bandit}(A)\n",
    "\\\\\n",
    "N(a) & \\leftarrow N(a) + 1\n",
    "\\\\\n",
    "Q(a) & \\leftarrow Q(a) + \\dfrac{1}{N(A)}\\big[R-Q(A)\\big]\n",
    "\\end{aligned}  \n",
    "$$\n",
    "***************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Incremental Implementation\n",
    "- Let $R_i$ is reward recieved after the $i$th selection\n",
    "- Let $Q_{n+1}$ is the estimated value after action has been selected $n$ times (for *stationnary* problem)\n",
    "$$Q_{n+1}=\\dfrac{1}{n}\\sum_{i=1}^nR_i$$\n",
    "- Rewrite $Q_{n+1}$ in form of incremental formulas\n",
    "$$Q_{n+1}=Q_n + \\dfrac{1}{n}\\big[R_n-Q_n\\big]$$\n",
    "\n",
    "- **General form**:\n",
    "\n",
    "$$NewEstimate \\leftarrow OldEstimate + StepSize\\big[Target - OldEstimate\\big]$$\n",
    "\n",
    "- $\\big[Target - OldEstimate\\big]$ is an **error** in the estimate\n",
    "- $StepSize$ is denoted as $\\alpha$ or, more generally by $\\alpha_t(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracking a Nonstationary Problem\n",
    "- **Nonstationnary**: Reward probabilities are changed over time\n",
    "- Use *exponental recency-weighted average*:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{n+1} &= Q_n+\\alpha\\big[Q_n\\big]\n",
    "\\\\\n",
    "& = (1-\\alpha)^nQ_1 + \\sum_{i=1}^n \\alpha(1-\\alpha)^{n-i}R_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "where constant step-size $\\alpha\\in(0, 1]$\n",
    "\n",
    "- Can vary the step-size parameter from step to step as $\\alpha_n(a)$\n",
    "- Stochastic approximation theory assures convergence with probability 1:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\displaystyle\\sum_{n=1}^{\\infty}\\alpha_n(a) = \\infty\n",
    "\\\\\n",
    "\\displaystyle\\sum_{n=1}^{\\infty}\\alpha_n^2(a) < \\infty\n",
    "\\end{cases}\n",
    "$$\n",
    "- The sample-average with $\\alpha_n(a)=\\dfrac{1}{n}$ will be convergenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimistic Initial Values\n",
    "- Bias $Q_1(a)$ for all methods above\n",
    "- Bias decreases over time because of $(1-\\alpha)<1$\n",
    "- Bias is a very helpful initial values as prior knowledge\n",
    "- First, setting $bias > 0$, will explore more, then the exploration is decreased with time\n",
    "- Quite effective on stationary problems, but not for nonstationnary problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upper-Confidence-Bound Action Selection\n",
    "- Select actions with largest upper confidence bound (**UCB**)\n",
    "$$A_t = \\arg\\max\\limits_a \\Bigg[ Q_t(a) + c\\sqrt{\\dfrac{\\ln t}{N_t(a)}} \\Bigg]$$\n",
    "\n",
    "  where $c>0$ controls the degree of exploration\n",
    "  \n",
    "- Reduce exporation over time\n",
    "- Square-root term is measure of the uncetainty or variance in the estimate of *a*'s value\n",
    "- $c$ determines the confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gradient Bandit Algorithms\n",
    "- Learn a numerical **preference** $H_t(a)$\n",
    "- Probability of selection action $a$ by soft-max distribution:\n",
    "$$Pr\\{A_t=a\\}=\\dfrac{\\exp\\big(H_t(a)\\big)}{\\sum_{b=1}^k\\exp\\big(H_t(b)\\big)}=\\pi_t(a)$$\n",
    "- Initially, all action preferences are the same, e.g: $\\forall a, ~~~ H_1(a)=0$\n",
    "- Preferences are updated by:\n",
    "$$H_{t+1}(a)=H_t(a)+\\alpha\\big(R_t-\\overline R_t\\big)\\big(\\mathbb 1_{A_t=a}-\\pi_t(a)\\big)$$\n",
    "\n",
    "    where, the baseline can be $\\overline R_t = \\displaystyle\\frac{1}{n}\\sum_{i=1}^t R_i$, not very best, but simple and works well in practice.\n",
    "    \n",
    "    - $Pr\\{A_t=a\\} \\varpropto \\big(R_t-\\overline R_t\\big)$\n",
    "    - non-selected actions move in the opposite direction of selected action\n",
    "    - $\\overline R_t$ can be computed incrementally by *5. Incremental Implementation*, or by *6. Tracking a Nonstationary Problem* for nonstationary problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Associative Search (Contextual Bandits)\n",
    "- Need to associate different actions with different situations\n",
    "- Involves both trial-and-error learning to *search* for the best actions, and *association* of these actions with the situations in which they are best.\n",
    "- Each action affects only the immediate reward\n",
    "    - *full* RL: actions effect the next situation as well as the reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "![summary](assets/bandit-summary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
