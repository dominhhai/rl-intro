{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *k*-armed Bandit Problem\n",
    "- Simplest RL problem with only single state\n",
    "- Set of `k` options (*actions*)\n",
    "- At each time step $t$, choose an *action* $A_t$, then receive a *reward* $R_t \\in \\mathbb R$\n",
    "- Expected reward (true *value* ) of action $a$ is $q_*(a)=E[R_t | A_t=a]$\n",
    "- The true values and distribution are unknown\n",
    "- Need estimate with estimated value $Q_t(a) \\approx q_*(a)$\n",
    "- Goal is maximize the expected total reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration vs Exploitation\n",
    "- Greedy Action at time $t$ is $A_t^* =\\arg\\max\\limits_a Q_t(a)$\n",
    "- *Exploiting* if $A_t = A_t^*$\n",
    "- *Exploring* if $A_t \\neq A_t^*$\n",
    "- Exploitaion maximizes the expected reward on the one step\n",
    "- Exploration may produce the greater total reward in the long run\n",
    "- Can't do both with any single action selection\n",
    "- Need to balance Exploitation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action-value Methods\n",
    "- Estimate the values of actions and use the estimates to make action selection decisions\n",
    "- *sample-average* method:\n",
    "$$Q_t(a)=\\dfrac{\\sum_{i=1}^{t-1}R_i \\cdot \\mathbb 1_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathbb 1_{A_i=a}}$$\n",
    "- $Q_t(a)$ coverages to $q_*(a)$ by the law of large numbers :\n",
    "$$\\lim\\limits_{N_t(a)\\rightarrow\\infty}Q_t(a)=q_*(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ε-greedy Methods\n",
    "- Usually select greedy actions\n",
    "- Random pick an action (includes non-greedy actions) with probability `ε`\n",
    "- Every action may be selected, all the $Q_t(a)$ can coverage to $q_*(a)$\n",
    "- Possible to reduce `ε` over time to try to get the best of both high and low values\n",
    "\n",
    "***************************************\n",
    "Initialize, for $a = 1$ to $k$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(a) & \\leftarrow 0\n",
    "\\\\\n",
    "N(a) & \\leftarrow 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Loop forever:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A & \\leftarrow\n",
    "    \\begin{cases}\n",
    "        \\arg\\max_a Q(a) &\\text{with probability }(1-\\epsilon)\n",
    "        \\\\\n",
    "        \\text{a random action} &\\text{with probability }\\epsilon\n",
    "    \\end{cases}\n",
    "\\\\\n",
    "R & \\leftarrow \\text{bandit}(A)\n",
    "\\\\\n",
    "N(a) & \\leftarrow N(a) + 1\n",
    "\\\\\n",
    "Q(a) & \\leftarrow Q(a) + \\dfrac{1}{N(A)}\\big[R-Q(A)\\big]\n",
    "\\end{aligned}  \n",
    "$$\n",
    "***************************************\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Incremental Implementation\n",
    "- Let $R_i$ is reward recieved after the $i$th selection\n",
    "- Let $Q_{n+1}$ is the estimated value after action has been selected $n$ times\n",
    "$$Q_{n+1}=\\dfrac{1}{n}\\sum_{i=1}^nR_i$$\n",
    "- Rewrite $Q_{n+1}$ in form of incremental formulas\n",
    "$$Q_{n+1}=Q_n + \\dfrac{1}{n}\\big[R_n-Q_n\\big]$$\n",
    "\n",
    "- **General form**:\n",
    "\n",
    "$$NewEstimate \\leftarrow OldEstimate + StepSize\\big[Target - OldEstimate\\big]$$\n",
    "\n",
    "- $\\big[Target - OldEstimate\\big]$ is an **error** in the estimate\n",
    "- $StepSize$ is denoted as $\\alpha$ or, more generally by $\\alpha_t(a)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tracking a Nonstationary Problem\n",
    "- **Nonstationnary**: Reward probabilities are changed over time\n",
    "- Use *exponentail recency-weighted average*:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{n+1} &= Q_n+\\alpha\\big[Q_n\\big]\n",
    "\\\\\n",
    "& = (1-\\alpha)^nQ_1 + \\sum_{i=1}^n \\alpha(1-\\alpha)^{n-i}R_i\n",
    "\\end{aligned}\n",
    "$$\n",
    "where constant step-size $\\alpha\\in(0, 1]$\n",
    "\n",
    "- Can vary the step-size parameter from step to step as $\\alpha_n(a)$\n",
    "- Stochastic approximation theory assures convergence with probability 1:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\displaystyle\\sum_{n=1}^{\\infty}\\alpha_n(a) = \\infty\n",
    "\\\\\n",
    "\\displaystyle\\sum_{n=1}^{\\infty}\\alpha_n^2(a) < \\infty\n",
    "\\end{cases}\n",
    "$$\n",
    "- The sample-average with $\\alpha_n(a)=\\dfrac{1}{n}$ will be convergenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Optimistic Initial Values\n",
    "- Bias $Q_1(a)$ for all methods above\n",
    "- Bias decreases over time because of $(1-\\alpha)<1$\n",
    "- Bias is a very helpful initial values as prior knowledge\n",
    "- First, setting $bias > 0$, will explore more, then the exploration is decreased with time\n",
    "- Quite effective on stationary problems, but not for nonstationnary problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Upper-Confidence-Bound Action Selection\n",
    "- Select actions with largest upper confidence bound (**UCB**)\n",
    "$$A_t = \\arg\\max\\limits_a \\Bigg[ Q_t(a) + c\\sqrt{\\dfrac{\\ln t}{N_t(a)}} \\Bigg]$$\n",
    "\n",
    "  where $c>0$ controls the degree of exploration\n",
    "  \n",
    "- Reduce exporation over time\n",
    "- Square-root term is measure of the uncetainty or variance in the estimate of *a*'s value\n",
    "- $c$ determines the confidence level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Gradient Bandit Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Associative Search (Contextual Bandits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
