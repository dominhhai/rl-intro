{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2: Multi-armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. *k*-armed Bandit Problem\n",
    "- Simplest RL problem with only single state\n",
    "- Set of `k` options (*actions*)\n",
    "- At each time step $t$, choose an *action* $A_t$, then receive a *reward* $R_t \\in \\mathbb R$\n",
    "- Expected reward (true *value* ) of action $a$ is $q_*(a)=E[R_t | A_t=a]$\n",
    "- The true values and distribution are unknown\n",
    "- Need estimate with estimated value $Q_t(a) \\approx q_*(a)$\n",
    "- Goal is maximize the expected total reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploration vs Exploitation\n",
    "- Greedy Action at time $t$ is $A_t^* =\\arg\\max\\limits_a Q_t(a)$\n",
    "- *Exploiting* if $A_t = A_t^*$\n",
    "- *Exploring* if $A_t \\neq A_t^*$\n",
    "- Exploitaion maximizes the expected reward on the one step\n",
    "- Exploration may produce the greater total reward in the long run\n",
    "- Can't do both with any single action selection\n",
    "- Need to balance Exploitation and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action-value Methods\n",
    "- Estimate the values of actions and use the estimates to make action selection decisions\n",
    "- *sample-average* method:\n",
    "$$Q_t(a)=\\dfrac{\\sum_{i=1}^{t-1}R_i \\cdot \\mathbb 1_{A_i=a}}{\\sum_{i=1}^{t-1}\\mathbb 1_{A_i=a}}$$\n",
    "- $Q_t(a)$ coverages to $q_*(a)$ by the law of large numbers :\n",
    "$$\\lim\\limits_{N_t(a)\\rightarrow\\infty}Q_t(a)=q_*(a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ε-greedy Methods\n",
    "- Usually select greedy actions\n",
    "- Random pick an action (includes non-greedy actions) with probability `ε`\n",
    "- Every action may be selected, all the $Q_t(a)$ can coverage to $q_*(a)$\n",
    "- Possible to reduce `ε` over time to try to get the best of both high and low values\n",
    "\n",
    "***************************************\n",
    "Initialize, for $a = 1$ to $k$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(a) & \\leftarrow 0\n",
    "\\\\\n",
    "N(a) & \\leftarrow 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Loop forever:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "A & \\leftarrow\n",
    "    \\begin{cases}\n",
    "        \\arg\\max_a Q(a) &\\text{with probability }(1-\\epsilon)\n",
    "        \\\\\n",
    "        \\text{a random action} &\\text{with probability }\\epsilon\n",
    "    \\end{cases}\n",
    "\\\\\n",
    "R & \\leftarrow \\text{bandit}(A)\n",
    "\\\\\n",
    "N(a) & \\leftarrow N(a) + 1\n",
    "\\\\\n",
    "Q(a) & \\leftarrow Q(a) + \\dfrac{1}{N(A)}[R-Q(A)]\n",
    "\\end{aligned}  \n",
    "$$\n",
    "***************************************\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
