{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9: On-policy Prediction with Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models and Planning\n",
    "- **model-based** methods:\n",
    "    - require a model of enviroment (DP, HS)\n",
    "    - rely on **planning**\n",
    "- **model-free** methods:\n",
    "    - does not require a model of enviroment (MC, TD)\n",
    "    - rely on **learning**\n",
    "- heart of 2 methods is the computation of value functions\n",
    "\n",
    "\n",
    "- **Model**: anything that an agent can use to predict how the environment will repond to its actions\n",
    "    - **Distribution Models**: description of all possibilities and their probabilites, $p(s',r | s,a) ~~~\\forall s,a,s',r$\n",
    "    - **Sample Models**: produce just one of the possibilities (sample experiences for given $s,a$)\n",
    "    - Model is used to *simulate* the environment and produce *simulated experience*\n",
    "        - distribution models are stronger\n",
    "        - however, easier to obtain sample models\n",
    "        \n",
    "\n",
    "- **Planning**: any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment\n",
    "    - **state-space** planning\n",
    "    - **plan-space** planning: difficult to apply stochastic squential decision problems\n",
    "\n",
    "![planning](assets/8.1.planning.png)\n",
    "\n",
    "- state-space planning methods view\n",
    "    - compute value functions in order to improve the policy\n",
    "    - apply the backup operations to simulated experience\n",
    "\n",
    "![state-space planning](assets/8.1.state-space-planning.png)\n",
    "\n",
    "- *planning* methods use simulated experience generated by a model\n",
    "- *learning* methods use real exeperience generated by the environment\n",
    "    - *random-sample one-step tabular Q-planning*\n",
    "![Q-planning](assets/8.1.q-planning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dyna: Integrated Planning, Acting, and Learning\n",
    "- 2 roles of real experience:\n",
    "    - **model learning**: improve the model\n",
    "        - also, called **indirect RL**\n",
    "    - **direct RL**: directyly improve the value funciton and policy\n",
    "\n",
    "![Experience Relationship](assets/8.2.exp-relation.png)\n",
    "\n",
    "\n",
    "- *indirect RL* (model-based)\n",
    "    - fuller use of a limited amount of experience, thus achieve a better policy with fewer environmental interactions\n",
    "- *direct RL* (model-free)\n",
    "    - much simpler, not affected by biases of the designed model\n",
    "\n",
    "\n",
    "- planning, acting, model-learning, and direct RL occur simultaneously and in paralel in Dyna agents\n",
    "    - Dyna Architecture\n",
    "\n",
    "![Dyna Architecture](assets/8.2.dyna-architecture.png)\n",
    "\n",
    "- **Dyna-Q** algorithm\n",
    "    - direct RL: step (d)\n",
    "    - model-learning, and planning: steps (e), and (f)\n",
    "\n",
    "![Dyna-Q](assets/8.2.dyna-q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. When the Model is Wrong\n",
    "- model may be incorrect\n",
    "    - environment is stochastic and only limited number of samples\n",
    "    - model learning function has generalized imperfectly\n",
    "    - environment has changed and its new behavior has not yet been observed\n",
    "- when model is incorrect, suboptimal policy is computed\n",
    "    - in some cases, this lead to discovery & correction of the modeling error\n",
    "- the general problem is the conflict between exploration and exploitation\n",
    "    - probably is no solution is perfect\n",
    "    - in practical, simple heuristics are often effective\n",
    "- **Dyna-Q+** method:\n",
    "    - state-action pair is not visted in $\\tau$ time steps\n",
    "    - add bones reward: $r + \\kappa\\sqrt{\\tau}$, for some small $\\kappa$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prioritized Sweeping\n",
    "- started state-action pairs selection by uniform is usually not the best, should focus on particular state-action pairs\n",
    "- work back from any state whose value has changed\n",
    "    - using queue to maintain every state-action pair whose estimated value would change nontrivially if updated\n",
    "    - prioriteize by the size of the change\n",
    "\n",
    "- waste lots of computation on low-probability transitions\n",
    "\n",
    "![Prioritized Sweeping](assets/8.4.prioritized_sweeping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exepected vs Sample Updates\n",
    "- for one-step updates, vary primarily along 2 binary dimensions\n",
    "    - state values or action values\n",
    "    - optimal policy or arbitrary given policy\n",
    "    - expected updates or sample updates\n",
    "\n",
    "![Backup Diagrams for one-step](assets/8.5.backup-diagrams.png)\n",
    "\n",
    "\n",
    "- expected updates are better but require more computation\n",
    "    - let $b$ is *branching factor*, expected update requires roughly $b$ times as much computation as a sample update\n",
    "- in a large problem, sample updates are preferable\n",
    "\n",
    "![Expected vs Sample updates](assets/8.5.expected_vs_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trajectory Sampling\n",
    "- **Trajectory Sampling**: simulates explicit individual trajectories and performs updates at the state or state-action pairs encountered along the way\n",
    "- Seem both efficient and elegant\n",
    "- Sampling according to the **on-policy** distribution\n",
    "    - faster planning initially and retarded planning in the long run\n",
    "    - in the long run, may hurt, sampling other states may useful\n",
    "    - for large problems, can be great advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-time Dynamic Programming - *RTDP*\n",
    "- An on-policy trajectory-sampling version of the value-interation algorithm of DP\n",
    "    - an example of an asynchronous DP algorithm\n",
    "- Allow completely skip states that cannot be reached by the given policy from any of the start states (*irrelevant*)\n",
    "    - can find a optimal policy on the relevant states without visting every states as *Sarsa*\n",
    "    - Greate advantage for very large state sets\n",
    "- Select a greedy action\n",
    "    - value function approaches the optimal value function $v_*$\n",
    "    - policy used by the agent to generate trajectories approaches an optimal policy\n",
    "- Strongly focused on subsets of the states that were relevant to the problem's objective\n",
    "- Reduce 50% of computation required by sweep-based value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Planning at Decision Time\n",
    "- 2 ways planning:\n",
    "    - **background planning**:\n",
    "        - use to gradually improve a policy or value function on the basis of simulated experience obtained from a model (such as DP an Dyna)\n",
    "        - not focus on the current state\n",
    "    - **decision-time planning**:\n",
    "        - use to begin and complete it after encountering each new state $S_t$\n",
    "        - focus on a particular state\n",
    "- in general, can mix both\n",
    "- most useful in applications in which fast responses are not required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Heuristic Search\n",
    "- A decision-time planning method\n",
    "- For each state encountered, a large tree of possible continuations is considered\n",
    "- Approximate value funciton is applied to the leaf nodes, then backed up toward the current state at root\n",
    "    - Backing up is just the same as in the expected updates with maxes ($v_*, q_*$)\n",
    "    - Backing up stops at the state-action nodes for the current state\n",
    "- Once the backed-up values of these nodes are computed\n",
    "    - The best of them is chosen as the current action\n",
    "    - All backed-up values are discarded\n",
    "- Can be viewed as an extension of the idea of a greedy policy, beyond a single step\n",
    "    - Seaching deeper than one step is to obtain better action selections\n",
    "    - The deeper the search, the more computation is required, slower response time\n",
    "- Can be so effective, because of smart focusing on the states and actions that might immediately follow the current state\n",
    "\n",
    "\n",
    "- Method of heuristic search\n",
    "    - Contruct a search tree\n",
    "    - Perform the individual one-step updates from bottom up\n",
    "\n",
    "![Heuristic Search](assets/8.9.heuristic-search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Rollout Algorithms\n",
    "- An decision-time planning algorithm based on MC control\n",
    "    - Simulate trajectories that all begin at the current environment state\n",
    "    - Estimate action values $q_\\pi$ by averaging the returns of many simulated trajectories\n",
    "    - Action with highest estimated value is executed\n",
    "    \n",
    "- The goal\n",
    "    - Not estimate a complete optimal action-value $q_*$ or a complete action-value $q_\\pi$ for a given policy $\\pi$\n",
    "    - Produce MC estimates of action values only for each current state and for a given policy - **rollout policy**\n",
    "    - Improve upon the rollout policy; not to find an optimal policy\n",
    "- The better the rollout policy and the more accurate the value estimates, the better the policy produced\n",
    "- It is important to tradeoff\n",
    "    - better rollout polices require more time is needed to simulate enough trajectories\n",
    "    - Run many trials in parallel on separate processors\n",
    "    - Truncate the simulated trajectories, correcting the truncated returns by means of a stored evaluation function\n",
    "- Not a learning algorithms\n",
    "    - do not maintain long-term memories of values or policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monte Carlo Tree Search\n",
    "- A successful example of decision-time planning\n",
    "- Is a rollout algorithm enhanced by the addition of a means for accumulating value estimates from MC simulations\n",
    "- Use in game and single-agent with simple model for fast multistep simulation\n",
    "- Execute after encountering each new state to select an action for that state\n",
    "    - Each execution is an iterative process that simulates many trajectories starting from the current state\n",
    "- Core idea is focus muliple simulations starting at the current state\n",
    "    - benfefits from online, incremental, sample-based value estimation and policy improvement\n",
    "    - can avoid the problem of globally approximating an action-value function while it retains the benefit of using past experience to guide exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary\n",
    "- 3 key ideas in common:\n",
    "    - Estimate value functions\n",
    "    - Operate by backing up values along actual or possible state trajectories\n",
    "    - Follow the general trategy of GPI (*generalized policy iteration*)\n",
    "\n",
    "![Space of RL](assets/8.11.space-of-rl.png)\n",
    "\n",
    "\n",
    "- 3th dimension: *on-policy* or *off-policy*\n",
    "- most important dimension: **function approximation** - in the part 2 of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
