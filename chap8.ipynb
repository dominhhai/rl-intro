{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Planning and Learning with Tabular Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models and Planning\n",
    "- **model-based** methods:\n",
    "    - require a model of enviroment (DP, HS)\n",
    "    - rely on **planning**\n",
    "- **model-free** methods:\n",
    "    - does not require a model of enviroment (MC, TD)\n",
    "    - rely on **learning**\n",
    "- heart of 2 methods is the computation of value functions\n",
    "\n",
    "\n",
    "- **Model**: anything that an agent can use to predict how the environment will repond to its actions\n",
    "    - **Distribution Models**: description of all possibilities and their probabilites, $p(s',r | s,a) ~~~\\forall s,a,s',r$\n",
    "    - **Sample Models**: produce just one of the possibilities (sample experiences for given $s,a$)\n",
    "    - Model is used to *simulate* the environment and produce *simulated experience*\n",
    "        - distribution models are stronger\n",
    "        - however, easier to obtain sample models\n",
    "        \n",
    "\n",
    "- **Planning**: any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment\n",
    "    - **state-space** planning\n",
    "    - **plan-space** planning: difficult to apply stochastic squential decision problems\n",
    "\n",
    "![planning](assets/8.1.planning.png)\n",
    "\n",
    "- state-space planning methods view\n",
    "    - compute value functions in order to improve the policy\n",
    "    - apply the backup operations to simulated experience\n",
    "\n",
    "![state-space planning](assets/8.1.state-space-planning.png)\n",
    "\n",
    "- *planning* methods use simulated experience generated by a model\n",
    "- *learning* methods use real exeperience generated by the environment\n",
    "    - *random-sample one-step tabular Q-planning*\n",
    "![Q-planning](assets/8.1.q-planning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dyna: Integrated Planning, Acting, and Learning\n",
    "- 2 roles of real experience:\n",
    "    - **model learning**: improve the model\n",
    "        - also, called **indirect RL**\n",
    "    - **direct RL**: directyly improve the value funciton and policy\n",
    "\n",
    "![Experience Relationship](assets/8.2.exp-relation.png)\n",
    "\n",
    "\n",
    "- *indirect RL* (model-based)\n",
    "    - fuller use of a limited amount of experience, thus achieve a better policy with fewer environmental interactions\n",
    "- *direct RL* (model-free)\n",
    "    - much simpler, not affected by biases of the designed model\n",
    "\n",
    "\n",
    "- planning, acting, model-learning, and direct RL occur simultaneously and in paralel in Dyna agents\n",
    "    - Dyna Architecture\n",
    "\n",
    "![Dyna Architecture](assets/8.2.dyna-architecture.png)\n",
    "\n",
    "- **Dyna-Q** algorithm\n",
    "    - direct RL: step (d)\n",
    "    - model-learning, and planning: steps (e), and (f)\n",
    "\n",
    "![Dyna-Q](assets/8.2.dyna-q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. When the Model is Wrong\n",
    "- model may be incorrect\n",
    "    - environment is stochastic and only limited number of samples\n",
    "    - model learning function has generalized imperfectly\n",
    "    - environment has changed and its new behavior has not yet been observed\n",
    "- when model is incorrect, suboptimal policy is computed\n",
    "    - in some cases, this lead to discovery & correction of the modeling error\n",
    "- the general problem is the conflict between exploration and exploitation\n",
    "    - probably is no solution is perfect\n",
    "    - in practical, simple heuristics are often effective\n",
    "- **Dyna-Q+** method:\n",
    "    - state-action pair is not visted in $\\tau$ time steps\n",
    "    - add bones reward: $r + \\kappa\\sqrt{\\tau}$, for some small $\\kappa$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prioritized Sweeping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exepected vs Sample Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trajectory Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Real-time Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Planning at Decision Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Heuristic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Rollout Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
