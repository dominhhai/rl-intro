{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "- Do not assume complete knownledge of the environment\n",
    "- Learning from experience of interaction with environment\n",
    "    - Experience is divided into **episodes**\n",
    "    - Based on averaging sample returns - complete returns of each episode\n",
    "- Like an associative bandit\n",
    "    - Nonstationary from the point of view of the earlier state\n",
    "- Adapt the idea of GPI from DP"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Prediction\n",
    "- Learning the state-value function $v_\\pi(s)$ for a given policy $\\pi$\n",
    "- Estimate from experience by averaging the returns observed after visits to that state $s$\n",
    "- a **visit** to $s$: each occurrence of state $s$ in an episode\n",
    "- 2 methods:\n",
    "    - **first-visit** MC: average of the returns only for the first visits to $s$\n",
    "    - **every-visit** MC: average of the returns for all visits to $s$\n",
    "\n",
    "- Only on choice considered at each state (unlike DP) - only sampled on he one episode\n",
    "- Estimates for each state are independent\n",
    "    - computational expense of estimating a single state is independent of the number of states\n",
    "- Do not bootstrap (unlike DP)\n",
    "\n",
    "- first-visit MC algorithm\n",
    "\n",
    "![First-Visit MC](assets/5.1.first-visit-mc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Estimation of Action Values\n",
    "- if a model is not available, MC is useful to estimate action values $q_*$\n",
    "- Averaging returns starting from state $s$, taking action $a$ following  policy $\\pi$\n",
    "- a **visit** to pair $s,a$: each occurrence of state $s$ and action $a$ is taken in it,  in an episode\n",
    "- 2 methods:\n",
    "    - first-visit MC: average of the returns only for the first visits to $s, a$\n",
    "    - every-visit MC: average of the returns for all visits to $s, a$\n",
    "- Need to estimate the value of all the actions from each state\n",
    "- **Exploring starts**: every pairs $s,a$ has nonzero probability of being selected as the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo Control\n",
    "- Use GPI\n",
    "$$\\pi_0 \\stackrel{E}{\\longrightarrow} q_{\\pi_0} \\stackrel{I}{\\longrightarrow} \\pi_1 \\stackrel{E}{\\longrightarrow} q_{\\pi_1} \\stackrel{I}{\\longrightarrow} \\pi_2 \\stackrel{E}{\\longrightarrow} ... \\stackrel{I}{\\longrightarrow} \\pi_* \\stackrel{E}{\\longrightarrow} q_{\\pi_*}$$\n",
    "\n",
    "- Policy evaluation $\\stackrel{E}{\\longrightarrow}$: using MC methods for prediction\n",
    "- Policy improment $\\stackrel{I}{\\longrightarrow}$: policy greedy with respect to the current value function\n",
    "    - meets the conditions for policy improvement by policy improvement theorem\n",
    "    - if, $\\pi_{k+1}=\\pi_k$, then $\\pi_k=\\pi_*$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi_k}\\big(s,\\pi_{k+1}(s)\\big) &= q_{\\pi_k}\\big(s,\\arg\\max_a q_{\\pi_k}(s,a)\\big)\n",
    "\\\\ &= \\max_a q_{\\pi_k}(s,a)\n",
    "\\\\ &\\ge q_{\\pi_k}\\big(s,\\pi_k(s)\\big)\n",
    "\\\\ &\\ge v_{\\pi_k}(s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Converage conditions assumptions:\n",
    "    - (1) episodes have exploring starts\n",
    "    - (2) policy evaluation could be done with an infinite number of episodes\n",
    " \n",
    "- Need to remove both assumptions in order to obtain a practical algorithm\n",
    "- Solve the assumption (2):\n",
    "    - Obtain bounds on the magnitude and probability of error in the estimates, assure that these bounds are sufficiently small\n",
    "    - Value Iteration\n",
    "    - Alterate between improvement and evaluation steps for single states\n",
    "    \n",
    "- Monte Carlo Exploring Starts (*Monte Carlo ES*)\n",
    "    - alterate between evaluation and improvement on an episode-by-episode basis\n",
    "    - convergence to this fixed point (fixed point is optimal policy $\\pi_*$) seems inevitable\n",
    "\n",
    "![Monte Carlo Exploring Starts](assets/5.3.mc-es.png)\n",
    "\n",
    "- Monte Carlo without Exploring Starts\n",
    "    - **on-policy** methods: evaluate or improve the policy that is used to make decisions\n",
    "    - **off-policy** methods: evaluate or improve the policy different from that is used to generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. On-Policy method\n",
    "- Learn about policy currently executing\n",
    "- Policy is generally soft $\\pi(a | s) > 0$\n",
    "- ε-soft policy like ε-greedy:\n",
    "    - probability of nongreedy is $\\dfrac{\\epsilon}{| \\mathcal A(s) |}$\n",
    "    - and, probability of greedy is $1-\\epsilon+\\dfrac{\\epsilon}{| \\mathcal A(s) |}$\n",
    "\n",
    "- ε-greedy with respect to $q_\\pi$ is an improvement over any ε-soft policy $\\pi$ is assured by the policy improvement theorem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_\\pi\\big(s,\\pi'(s)\\big) &= \\sum_a \\pi'(a | s) q_\\pi(s,a)\n",
    "\\\\ &= \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) + (1-\\epsilon)\\max_a q_\\pi(s,a)\n",
    "\\\\ &\\ge \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) + (1-\\epsilon)\\sum_a \\frac{\\pi(a | s)-\\frac{\\epsilon}{| \\mathcal A(s) |}}{1-\\epsilon} q_\\pi(s,a)\n",
    "\\\\ &= \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) - \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) + \\sum_a \\pi(a | s) q_\\pi(s,a)\n",
    "\\\\ &\\ge v_\\pi(s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Converages to best ε-soft policy $v_\\pi=\\tilde v_* ~~~, \\forall s\\in\\mathcal S$\n",
    "\n",
    "![On-Policy e-soft](assets/5.4.e-soft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Off-policy method\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
