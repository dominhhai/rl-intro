{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: Monte Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "- Do not assume complete knownledge of the environment\n",
    "- Learning from experience of interaction with environment\n",
    "    - Experience is divided into **episodes**\n",
    "    - Based on averaging sample returns - complete returns of each episode\n",
    "- Like an associative bandit\n",
    "    - Nonstationary from the point of view of the earlier state\n",
    "- Adapt the idea of GPI from DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Monte Carlo Prediction\n",
    "- Learning the state-value function $v_\\pi(s)$ for a given policy $\\pi$\n",
    "- Estimate from experience by averaging the returns observed after visits to that state $s$\n",
    "- a **visit** to $s$: each occurrence of state $s$ in an episode\n",
    "- 2 methods:\n",
    "    - **first-visit** MC: average of the returns only for the first visits to $s$\n",
    "    - **every-visit** MC: average of the returns for all visits to $s$\n",
    "\n",
    "- Only on choice considered at each state (unlike DP) - only sampled on the one episode\n",
    "- Estimates for each state are independent\n",
    "    - computational expense of estimating a single state is independent of the number of states\n",
    "- Do not bootstrap (unlike DP)\n",
    "\n",
    "- first-visit MC algorithm\n",
    "\n",
    "![First-Visit MC](assets/5.1.first-visit-mc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Monte Carlo Estimation of Action Values\n",
    "- if a model is not available, MC is useful to estimate action values $q_*$\n",
    "- Averaging returns starting from state $s$, taking action $a$ following  policy $\\pi$\n",
    "- a **visit** to pair $s,a$: each occurrence of state $s$ and action $a$ is taken in it,  in an episode\n",
    "- 2 methods:\n",
    "    - first-visit MC: average of the returns only for the first visits to $s, a$\n",
    "    - every-visit MC: average of the returns for all visits to $s, a$\n",
    "- Need to estimate the value of all the actions from each state\n",
    "- **Exploring starts**: every pairs $s,a$ has nonzero probability of being selected as the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Monte Carlo Control\n",
    "- Use GPI\n",
    "$$\\pi_0 \\stackrel{E}{\\longrightarrow} q_{\\pi_0} \\stackrel{I}{\\longrightarrow} \\pi_1 \\stackrel{E}{\\longrightarrow} q_{\\pi_1} \\stackrel{I}{\\longrightarrow} \\pi_2 \\stackrel{E}{\\longrightarrow} ... \\stackrel{I}{\\longrightarrow} \\pi_* \\stackrel{E}{\\longrightarrow} q_{\\pi_*}$$\n",
    "\n",
    "- Policy evaluation $\\stackrel{E}{\\longrightarrow}$: using MC methods for prediction\n",
    "- Policy improment $\\stackrel{I}{\\longrightarrow}$: policy greedy with respect to the current value function\n",
    "    - meets the conditions for policy improvement by policy improvement theorem\n",
    "    - if, $\\pi_{k+1}=\\pi_k$, then $\\pi_k=\\pi_*$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{\\pi_k}\\big(s,\\pi_{k+1}(s)\\big) &= q_{\\pi_k}\\big(s,\\arg\\max_a q_{\\pi_k}(s,a)\\big)\n",
    "\\\\ &= \\max_a q_{\\pi_k}(s,a)\n",
    "\\\\ &\\ge q_{\\pi_k}\\big(s,\\pi_k(s)\\big)\n",
    "\\\\ &\\ge v_{\\pi_k}(s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Converage conditions assumptions:\n",
    "    - (1) episodes have exploring starts\n",
    "    - (2) policy evaluation could be done with an infinite number of episodes\n",
    " \n",
    "- Need to remove both assumptions in order to obtain a practical algorithm\n",
    "- Solve the assumption (2):\n",
    "    - Obtain bounds on the magnitude and probability of error in the estimates, assure that these bounds are sufficiently small\n",
    "    - Value Iteration\n",
    "    - Alterate between improvement and evaluation steps for single states\n",
    "    \n",
    "- Monte Carlo Exploring Starts (*Monte Carlo ES*)\n",
    "    - alterate between evaluation and improvement on an episode-by-episode basis\n",
    "    - convergence to this fixed point (fixed point is optimal policy $\\pi_*$) seems inevitable\n",
    "\n",
    "![Monte Carlo Exploring Starts](assets/5.3.mc-es.png)\n",
    "\n",
    "- Monte Carlo without Exploring Starts\n",
    "    - **on-policy** methods: evaluate or improve the policy that is used to make decisions\n",
    "    - **off-policy** methods: evaluate or improve the policy different from that is used to generate the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. On-Policy method\n",
    "- Learn about policy currently executing\n",
    "- Policy is generally soft $\\pi(a | s) > 0$\n",
    "- ε-soft policy like ε-greedy:\n",
    "    - probability of nongreedy is $\\dfrac{\\epsilon}{| \\mathcal A(s) |}$\n",
    "    - and, probability of greedy is $1-\\epsilon+\\dfrac{\\epsilon}{| \\mathcal A(s) |}$\n",
    "\n",
    "- ε-greedy with respect to $q_\\pi$ is an improvement over any ε-soft policy $\\pi$ is assured by the policy improvement theorem\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_\\pi\\big(s,\\pi'(s)\\big) &= \\sum_a \\pi'(a | s) q_\\pi(s,a)\n",
    "\\\\ &= \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) + (1-\\epsilon)\\max_a q_\\pi(s,a)\n",
    "\\\\ &\\ge \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) + (1-\\epsilon)\\sum_a \\frac{\\pi(a | s)-\\frac{\\epsilon}{| \\mathcal A(s) |}}{1-\\epsilon} q_\\pi(s,a)\n",
    "\\\\ &= \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) - \\frac{\\epsilon}{| \\mathcal A(s) |}\\sum_a q_\\pi(s,a) + \\sum_a \\pi(a | s) q_\\pi(s,a)\n",
    "\\\\ &\\ge v_\\pi(s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Converages to best ε-soft policy $v_\\pi=\\tilde v_* ~~~, \\forall s\\in\\mathcal S$\n",
    "\n",
    "![On-Policy e-soft](assets/5.4.e-soft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Off-policy method\n",
    "- learn the value of the **target policy** $\\pi$ from experience due to **behavior policy** $b$\n",
    "    - optimal policy: target policy\n",
    "    - exploratory & generate policy: behavior policy\n",
    "- more powerfull and general than on-policy\n",
    "- greater variance and slower to converge\n",
    "- on-policy methods is special case in which $\\pi = b$\n",
    "- Coverage assumption: $b$ generates behavior that covers, or includes, $\\pi$\n",
    "    $$\\pi(a | s) > 0 \\implies b(a | s) > 0$$\n",
    "- Method: use **importance sampling**\n",
    "    - estimate expected values under one distribution given samples from another\n",
    "    - **importance-sampling ratio**: weighting returns according to the relative probability of their trajectories under the two policies  \n",
    "- The relative probability of the trajectory under 2 polices depend only on the 2 policies and the sequence:\n",
    "    $$\\rho_{t:T-1} = \\prod_{k=1}^{T-1}\\frac{\\pi(A_k | S_k)}{b(A_k | S_k)}$$\n",
    "- Expected value of target policy:\n",
    "    $$v_\\pi(s) = E\\big[\\rho_{t:T-1}G_t | S_t=s\\big]$$\n",
    "    where, $G_t$ is returns of $b$ : $v_b(s) = E\\big[G_t | S_t=s\\big]$\n",
    "- indexing time steps in a way that increases across episode boundaries\n",
    "    - first episode ends in terminal state at time $t-1=100$\n",
    "    - next episode begins at time $t=101$\n",
    "- 2 types of importance sampling:\n",
    "    - **ordinary importance sampling**:\n",
    "        $$V(s) = \\frac{\\sum_{t\\in\\mathscr T(s)}\\rho_{t:T(t)-1}G_t}{| \\mathscr T |}$$\n",
    "    - **weighted importance sampling**:\n",
    "        $$V(s) = \\frac{\\sum_{t\\in\\mathscr T(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t\\in\\mathscr T(s)}\\rho_{t:T(t)-1}}$$\n",
    "        \n",
    "    where:\n",
    "        - $\\mathscr T(s)$: set of all time steps in which state $s$ is visited\n",
    "        - $T(t)$: first time of termination following time $t$\n",
    "        - $G_t$: returns after $t$ up through $T(t)$\n",
    "        - $\\{G_t\\}_{t\\in\\mathscr T(s)}$ are the returns that pertain to state $s$\n",
    "        - $\\{\\rho_{t:T(t)-1}\\}_{t\\in\\mathscr T(s)}$ are the corresponding importance-sampling ratios\n",
    "        \n",
    "- for the *first-visit* methods:\n",
    "    - ordinary importance sampling is unbiased and variance is unbounded\n",
    "    - weighted importance sampling is biased and variance is bound $[0, 1)$ (converges to zero)\n",
    "- for the *every-visit* methods:\n",
    "    - both biased\n",
    "    - bias falls asymptotically to zero as the number of samples increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Incremental Implementation for MC prediction\n",
    "- similar to Bandits ([Chap 2](chap2.ipynb#5.-Incremental-Implementation)) but for the average *returns* $G_t$ instead of average *rewards* $R_t$\n",
    "- for *on-policy*: exactly the same methods as Bandits\n",
    "- for *ordinary importance sampling*\n",
    "    - scaling returns by $\\rho_{t:T(t)-1}$\n",
    "- for *weighted importance sampling*\n",
    "    - have sequence of returns $G_1, G_2, ..., G_{n-1}$ all starting in the same state\n",
    "    - corresponding random weight $W_i$ (e.g., $W_i = \\rho_{t_i:T(t_i)-1}$)\n",
    "        $$V_n = \\frac{\\sum_{k=1}^{n-1}W_kG_k}{\\sum_{k=1}^{n-1}W_k} ~~~, n\\ge 2$$\n",
    "     - update rule:\n",
    "         $$\n",
    "         \\begin{cases}\n",
    "         V_{n+1} &= V_n + \\dfrac{W_n}{C_n}\\big[G_n - V_n\\big] ~~~, n\\ge 1\n",
    "         \\\\\n",
    "         C_{n+1} &= C_n + W_{n+1} ~~~, C_0 = 0\n",
    "         \\end{cases}\n",
    "         $$\n",
    "     - can apply to on-policy when $\\pi=b, W=1$\n",
    "     \n",
    "     ![Weighted Importance Sampling](assets/5.6.weighted-importance-sampling.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Off-policy MC Control\n",
    "- Requires behavior $b$ is soft: $b(a | s) > 0$\n",
    "- Advantage:\n",
    "    - target policy $\\pi$ may be deterministic (e.g., greedy)\n",
    "    - while the behavior policy $b$ can continue to sample all possible actions\n",
    "- Disadvantages:\n",
    "    - learn only from the tails of episodes (the remaining actions in the episode are greedy)\n",
    "    - Greatly slow learning when non-greedy actions are common (states appearing in the early portions of long episodes)\n",
    "\n",
    "![off-policy MC control](assets/5.7.off-policy-mc-control.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Discounting-aware Importance Sampling\n",
    "- Use discounted rewards structure of the returns to reduce the variance of off-policy estimators\n",
    "- **flat partial returns**:\n",
    "    $$\\overline G_{t:h} = R_{t+1} + R_{t+2} + ... + R_h ~~~, 0\\le t < h\\le T$$\n",
    "- full return $G_t$ by flat partial returns:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_t &= R_{t+1} + \\gamma R_{t+2}  + \\gamma^2 R_{t+3} + ... + \\gamma^{T-t-1} R_T\n",
    "\\\\ &= (1-\\gamma)R_{t+1}\n",
    "\\\\ & ~~~ + (1-\\gamma)\\gamma(R_{t+1} + R_{t+2})\n",
    "\\\\ & ~~~ + (1-\\gamma)\\gamma^2(R_{t+1} + R_{t+2} + R_{t+3})\n",
    "\\\\ & ~~~\\vdots\n",
    "\\\\ & ~~~ + (1-\\gamma)\\gamma^{T-t-2}(R_{t+1} + R_{t+2} + ... + R_{T-1})\n",
    "\\\\ & ~~~ + \\gamma^{T-t-1}(R_{t+1} + R_{t+2} + ... + R_T)\n",
    "\\\\ &= (1-\\gamma)\\sum_{h=t+1}^{T-1}\\gamma^{h-t-1}\\overline G_{t:h} + \\gamma^{T-t-1}\\overline G_{t:T}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- **discounting-aware** importance sampling\n",
    "    - discount rate but have no effect if $\\gamma=1$\n",
    "    - for ordinary importance-sampling estimator:\n",
    "      $$V(s)=\\dfrac{\\displaystyle\\sum_{t\\in\\mathscr T(s)}\\Big( (1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\gamma^{h-t-1}\\rho_{t:h-1}\\overline G_{t:h} + \\gamma^{T(t)-t-1}\\rho_{t:T(t)-1}\\overline G_{t:T(t)}\\Big)}{| \\mathscr T(s) |}$$\n",
    "    - for weighted importance-sampling estimator:\n",
    "       $$V(s)=\\dfrac{\\displaystyle\\sum_{t\\in\\mathscr T(s)}\\Big( (1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\gamma^{h-t-1}\\rho_{t:h-1}\\overline G_{t:h} + \\gamma^{T(t)-t-1}\\rho_{t:T(t)-1}\\overline G_{t:T(t)}\\Big)}{\\displaystyle\\sum_{t\\in\\mathscr T(s)}\\Big( (1-\\gamma)\\sum_{h=t+1}^{T(t)-1}\\gamma^{h-t-1}\\rho_{t:h-1} + \\gamma^{T(t)-t-1}\\rho_{t:T(t)-1}\\Big)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Per-decision Importance Sampling\n",
    "- One more way of reducing variance, even if $\\gamma=1$\n",
    "- Use structure of the returns as sum of rewards\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\rho_{t:T-1}G_t &= \\rho_{t:T-1}(R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{T-t-1} R_T)\n",
    "\\\\ &= \\rho_{t:T-1}R_{t+1}+\\gamma\\rho_{t:T-1}R_{t+2}+...+\\gamma^{T-t-1}\\rho_{t:T-1}R_T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    where, sub-term $\\rho_{t:T-1}R_{t+k}$ depend only on the first and the last rewards\n",
    "$$E[\\rho_{t:T-1}R_{t+k}] = E[\\rho_{t:t+k-1}R_{t+k}]$$\n",
    "\n",
    "- **per-decision** importance-sampling\n",
    "$$E[\\rho_{t:T-1}G_t] = E[\\tilde G_t]$$\n",
    "\n",
    "    where, $\\tilde G_t=\\rho_{t:t}R_{t+1} + \\gamma\\rho_{t:t+1}R_{t+2} + \\gamma^2\\rho_{t:t+2}R_{t+3} + ... + \\gamma^{T-t-1}\\rho_{t:T-1}R_T$\n",
    "\n",
    "\n",
    "- Use for *ordinary* importance-sampling\n",
    "    - same unbiased expectation (in the first-visit case) as the ordinary importance-sampling estimator\n",
    "    - but not consistent (do not converge to the true value with infinite data)\n",
    "    $$V(s)=\\frac{\\sum_{t\\in\\mathscr T(s)}\\tilde G_t}{| \\mathscr T(s) |}$$\n",
    "\n",
    "- NOT for weighted importance-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
