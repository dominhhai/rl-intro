{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exercise 4.1\n",
    "$\\pi$ is equiprobable random policy, so all actions equally likely.\n",
    "\n",
    "- $q_\\pi(11, down)$\n",
    "\n",
    "With current state $s=11$ and action $a=down$, we have next is the terminal state $s'=end$, which have reward $R'=0$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_\\pi(11, down) &= \\sum_{s',r}p(s',r | s,a)\\big[r+\\gamma v_\\pi(s')\\big]\n",
    "\\cr &= 1 * (-1 + 0)\n",
    "\\cr &= -1\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- $q_\\pi(7, down)$\n",
    "\n",
    "With current state $s=7$ and action $a=down$, we have next is the terminal state $s'=11$, which have state-value function $v_\\pi(s)$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_\\pi(7, down) &= \\sum_{s',r}p(s',r | s,a)\\big[r+\\gamma v_\\pi(s')\\big]\n",
    "\\cr &= 1 * \\big[-1 + \\gamma v_\\pi(s')\\big]\n",
    "\\cr &= -1 + \\gamma v_\\pi(s')\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise 4.2\n",
    "- Transitions from the original states are unchanged\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_\\pi(15) &= \\sum_a \\pi(a|s=15)\\sum_{s',r}p(s',r|s,a)\\big[r+\\gamma v_\\pi(s')\\big]\n",
    "\\cr &= 0.25\\big[1*\\big(-1+\\gamma v_\\pi(12)\\big)+1*\\big(-1+\\gamma v_\\pi(13)\\big)+1*\\big(-1+\\gamma v_\\pi(14)\\big)+1*\\big(-1+\\gamma v_\\pi(15)\\big)\\big]\n",
    "\\cr &= -1 + 0.25\\gamma\\sum_{s=12}^{15}v_\\pi(s)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In which, $\\displaystyle v_\\pi(13)=-1 + 0.25\\gamma\\sum_{s\\in\\{9,12,13,14\\}}v_\\pi(s)$ \n",
    "- Add action **down** to state 13, to go to state 15\n",
    "\n",
    "Compute Fomular is similar to above:\n",
    "$$v_\\pi(15)=-1 + 0.25\\gamma\\sum_{s=12}^{15}v_\\pi(s)$$\n",
    "\n",
    "But, $\\displaystyle v_\\pi(13)=-1 + 0.25\\gamma\\sum_{s\\in\\{9,12,14,15\\}}v_\\pi(s)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise 4.3\n",
    "- $q_\\pi$ evaluation\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_\\pi(s, a) &= E[G_t | S_t=s, A_t=a]\n",
    "\\cr &= E[R_{t+1}+\\gamma G_{t+1} | S_t=s, A_t=a]\n",
    "\\cr &= E[R_{t+1}+\\gamma V_\\pi(S_{t+1}) | S_t=s, A_t=a]\n",
    "\\cr &= \\sum_{s',r}p(s',r | s,a)\\big[r+\\gamma v_\\pi(s')\\big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- Update rule for $q_\\pi$\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q_{k+1}(s, a) &= E_\\pi[R_{t+1} + \\gamma v_k(S_{t+1}) | S_t=s, A_t=a]\n",
    "\\cr &= \\sum_{s',r}p(s',r | s,a)\\big[r+\\gamma v_k(s')\\big]\n",
    "\\cr &= \\sum_{s',r}p(s',r | s,a)\\Big[r+\\gamma \\sum_{a'}\\pi(a' | s')q_k(s', a')\\Big]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exercise 4.4\n",
    "When, the policy continually switches between two or more policies that are equally good, the difference between switches is small, so policy evaluation loop will be breaked before convergence.\n",
    "\n",
    "$$\\Delta = \\max\\big(\\Delta, | v-V(s) |\\big)$$\n",
    "\n",
    "So, in this case, it maybe useful if we talk the sum of all differences\n",
    "$$\\Delta = \\Delta + | v-V(s) |$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
