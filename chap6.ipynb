{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6: Temporal-Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TD Prediction\n",
    "- combination of MC and DP\n",
    "    - use experience\n",
    "    - bootstrapping\n",
    "- *constant-α* MC for simple every-visit MC method:\n",
    "    - must wait until the end of the episode to obtain $G_t$\n",
    "$$V(S_t) = V(S_t) + \\alpha\\big[G_t-V(S_t)\\big]$$\n",
    "- **one-step TD** - *TD(0)* method:\n",
    "    - bootstrapping\n",
    "$$V(S_t) = V(S_t) + \\alpha\\big[R_{t+1}+\\gamma V(S_{t+1})-V(S_t)\\big]$$\n",
    "\n",
    "![TD 0](assets/6.1.td0.png)\n",
    "\n",
    "\n",
    "- TD *error*:\n",
    "$$\\delta_t = R_{t+1}+\\gamma V(S_{t+1})-V(S_t)$$\n",
    "\n",
    "    if, array $V$ does not change during the episode, then MC error can be written as a sum of TD errors:\n",
    "    $$G_t-V(S_t) = \\sum_{k=t}^{T-1}\\gamma^{k-t}\\delta_k$$\n",
    "    \n",
    "- Advantages of TD prediction\n",
    "    - do not require a model of the environment, only experience\n",
    "    - can implement in an online, fully incremental fashion\n",
    "        - wait only one time step\n",
    "    - converge to the true values\n",
    "    - TD is faster than constant-α MC, in pratice\n",
    "        - open question for proving mathematically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Optimality of TD(0)\n",
    "- **Batch Updating**: only update after processing each complete *batch* of training data\n",
    "    - compute approximate value function incrementally by TD or MC\n",
    "    - only update value function by the sum of all the increments\n",
    "- TD(0) converges deterministically to a single answer if $\\alpha$ is sufficiently small\n",
    "- constant-α MC also converge with the same conditions but to a *different answer*\n",
    "- TD perform better for future, MC is better for past data\n",
    "    - TD finds *maximum-likelihood* model of the Markov process\n",
    "        - converge to the **certaintly-equivalence estimate** : if model is correct, estimating value function will be correct\n",
    "    - MC minimie *mean-squared error* on the training set\n",
    "- TD is faster than MC because it computes the true certaintly-equivalence estimate\n",
    "- TD methods can approximate the same solution as certaintly-equivalence estimate using less memory and computation cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sarsa: On-policy TD Control\n",
    "- Learn an action-value function $q_\\pi(s, a)$ for the current behavior policy\n",
    "![state-action pairs transition](assets/6.4.state-action-pairs.png)\n",
    "\n",
    "- TD(0) for action-value function:\n",
    "$$Q(S_t,A_t)\\gets Q(S_t,A_t)+\\alpha\\big[R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)\\big]$$\n",
    "    - SARSA: $S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}$\n",
    "  \n",
    "- Continually estimate $q_\\pi$ for the behavior policy $\\pi$, and change $\\pi$ toward greediness with respect to $q_\\pi$ at the same time\n",
    "\n",
    "![sarsa](assets/6.4.sarsa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q-learning: Off-policy TD Control\n",
    "- The learned action-value function $Q$, directly approximates the optimal $q_*$, independent of the policy being followed\n",
    "\n",
    "$$Q(S_t,A_t)\\gets Q(S_t,A_t)+\\alpha\\big[R_{t+1}+\\gamma\\max_aQ(S_{t+1},a)-Q(S_t,A_t)\\big]$$\n",
    "\n",
    "![Q-learning](assets/6.5.q-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expected Sarsa\n",
    "- Just like Q-learning exept that taking into account how likely each action is under the current policy\n",
    "- Instead of the sample value of next state, use the **expectation**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q(S_t,A_t) &\\gets Q(S_t,A_t)+\\alpha\\Big[R_{t+1}+\\gamma E_\\pi\\big[Q(S_{t+1},A_{t+1}) | S_{t+1}\\big]-Q(S_t,A_t)\\Big]\n",
    "\\\\ &\\gets Q(S_t,A_t)+\\alpha\\Big[R_{t+1}+\\gamma \\sum_a\\pi(a | S_{t+1})Q(S_{t+1},a)-Q(S_t,A_t)\\Big]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- E-Sarsa is more complex computationally than Sarsa\n",
    "-  but, perform better than Sarsa, because of eliminating the variance due to random selection of $A_{t+1}$\n",
    "- Might use for off-policy algorithm\n",
    "    - includes Q-learning as the sepcial case in which $\\pi$ is the greedy policy\n",
    "- E-Sarsa subsumes and generalizes Q-learning while reliably improving over Sarsa\n",
    "- Additional computationl cost, but completely dominate both of the other more-well-known TD control algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Maximization Bias and Double Learning\n",
    "- **Maxsimization Bias**: maximum over estimated values can lead to a significant positive bias\n",
    "- Avoid by **Double Learning**\n",
    "    - Divide the plays in two sets and learn independent estimates $Q_1(a)$, $Q_2(a)$\n",
    "    - Use one (e.x. $Q_1$) to get maximizing action: $A^*=\\arg\\max_a Q_1(a)$\n",
    "    - Other (e.x. $Q_2$) to provide the estimate of its value: $Q_2(A^*)=Q_2(\\arg\\max_a Q_1(a))$\n",
    "    - The result is that it will then unbiased: $E[Q_2(A^*)]=q(A^*)$\n",
    "- Note that:\n",
    "    - only one estimate is updated on each play\n",
    "    - double the memory requirements\n",
    "    - does not increase the amount of computation per step\n",
    "- *Double Q-learning*:\n",
    "$$Q_1(S_t,A_t)\\gets Q_1(S_t,A_t)+\\alpha\\big[R_{t+1}+\\gamma Q_2\\big(S_{t+1},\\arg\\max_aQ_1(S_{t+1}, a)\\big)-Q_1(S_t,A_t)\\big]$$\n",
    "\n",
    "![Double Q-learning](assets/6.7.double-q-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Games, Afterstates, and Other Special Cases\n",
    "- **Afterstates**: State after agent has acted\n",
    "    - value functions are **afterstate value functions**\n",
    "    - useful when we have knowledge of an initial part of the environment's dynamics\n",
    "- Move to *afterposition*  must have the same value, so can transfer to other pair have same afterstate without separately learn\n",
    "- Can use in GPI with a policy and afterstate value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
