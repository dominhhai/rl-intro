{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. n-step TD Prediction\n",
    "- Generalize one-step TD(0) method\n",
    "- Temporal difference extends over n-steps\n",
    "\n",
    "![n-step methods](assets/7.1.n-step.png)\n",
    "\n",
    "- Want to update estimated value $v_\\pi(S_t)$ of state $S_t$ from:\n",
    "$$S_t,R_{t+1},S_{t+1},R_{t+1},...,R_T,S_T$$\n",
    "\n",
    "    - for *MC*, target is complete return\n",
    "    $$G_t = R_{t+1}+\\gamma R_{t+3}+\\gamma^2R_{t+3}+...+\\gamma^{T-t-1}R_T$$\n",
    "    - for *TD*, one-step method\n",
    "    $$G_{t:t+1} = R_{t+1}+\\gamma V_t(S_{t+1})$$\n",
    "    - for *two-step TD*, one-step method\n",
    "    $$G_{t:t+2} = R_{t+1}+\\gamma R_{t+2}+\\gamma^2V_{t+1}(S_{t+2})$$\n",
    "    - for *n-step TD*, one-step method with $n\\ge 1, 0\\le t<T-n$\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "    G_{t:t+n} &= R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^nV_{t+n-1}(S_{t+n})\n",
    "    \\\\G_{t:t+n} &= G_t ~~~,\\text{if } t+n\\ge T\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "\n",
    "- Wait for $R_{t+n}, V_{t+n-1}$, until time $t+n$, then update estimate values:\n",
    "$$V_{t+n}(S_t) = V_{t+n-1}(S_t)+\\alpha\\big[G_{t:t+n}-V_{t+n-1}(S_t)\\big] ~~~, 0\\le t<T$$\n",
    "    - all other states remain unchanged: $V_{t+n}(s)=V_{t+n-1}(s), \\forall s\\neq S_t$\n",
    "\n",
    "![n-step methods](assets/7.1.n-step-td.png)\n",
    "\n",
    "\n",
    "- **Error Reduction Property** of n-step returns:\n",
    "$$\\max_s\\big| E_\\pi[G_{t:t+n} | S_t=s]-v_\\pi(s)\\big| \\le \\gamma^n\\max_s\\big| V_{t+n-1}(s)-v_\\pi(s)\\big|, \\forall n\\ge 1$$\n",
    "- Can show formally that n-step TD methods converge to the correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. n-step Sarsa\n",
    "- Switch states for actions (state-action pairs) and then use an ε-greedy policy\n",
    "\n",
    "![n-step SARSA](assets/7.2.n-step-sarsa.png)\n",
    "\n",
    "- n-step returns for action-value:\n",
    "$$G_{t:t+n}=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^nQ_{t+n-1}(S_{t+n},A_{t+n})~~~, n\\ge 1, 0\\le t<T-n$$\n",
    "    with $G_{t:t+n}=G_t \\text{ if }t+n\\ge T$\n",
    "\n",
    "\n",
    "- **n-step Sarsa**:\n",
    "    $$Q_{t+n}(S_t,A_t)=Q_{t+n-1}(S_t,A_t)+\\alpha\\big[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\\big]~~~,0\\le t<T$$\n",
    " \n",
    " \n",
    "- **n-step Expected Sarsa**:\n",
    "$$G_{t:t+n}=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^n\\overline V_{t+n-1}(S_{t+n},A_{t+n})~~~, t+n<T$$\n",
    "    - where, *expected approximate value* of state $s$:\n",
    "    $$\\overline V_t(s)=\\sum_a\\pi(a | s)Q_t(s,a) ~~~, \\forall s\\in\\mathcal S$$\n",
    "    - if $s$ is terminal, then $\\overline V(s)=0$\n",
    "    \n",
    "![n-step SARSA Pseudocode](assets/7.2.n-step-sarsa-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. n-step Off-policy Learning\n",
    "- Use relative probability of just n actions:\n",
    "$$\\rho_{t:h}=\\prod_{k=t}^{\\min(h,T-1)}\\frac{\\pi(A_k | S_k)}{b(A_k | S_k)}$$\n",
    "\n",
    "- n-step TD:\n",
    "$$V_{t+n}(S_t)=V_{t+n-1}(S_t)+\\alpha\\color{blue}{\\rho_{t:t+n-1}}\\big[G_{t:t+n}-V_{t+n-1}(S_t)\\big]~~~,0\\le t<T$$\n",
    "\n",
    "- n-step Sarsa:\n",
    "$$Q_{t+n}(S_t,A_t)=V_{t+n-1}(S_t,A_t)+\\alpha\\color{blue}{\\rho_{t+1:t+n}}\\big[G_{t:t+n}-Q_{t+n-1}(S_t,A_1)\\big]~~~,0\\le t<T$$\n",
    "\n",
    "- n-step Expected Sarsa:\n",
    "$$Q_{t+n}(S_t,A_t)=V_{t+n-1}(S_t,A_t)+\\alpha\\color{blue}{\\rho_{t+1:t+n-1}}\\big[G_{t:t+n}-Q_{t+n-1}(S_t,A_1)\\big]~~~,0\\le t<T$$\n",
    "\n",
    "![n-step Off-policy](assets/7.3.n-step-off-policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-decision Methods with Control Variates\n",
    "- add *control variate* to **off-policy** of n-step return to reduce variance\n",
    "$$G_{t:h}=\\rho_t(R_{t+1}+\\gamma G_{t+1:h})+(1-\\rho_t)V_{h-1}(S_t) ~~~,t<h<T$$\n",
    "    where, $G_{h:h}=V_{h-1}(S_h)$\n",
    "\n",
    "- if $\\rho_t=0$, then the target does not change\n",
    "- Includes on-policy when $\\rho_t=1$\n",
    "- for action values, the first action does not play a role in the importance sampling\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_{t:h} &= R_{t+1}+\\gamma\\big(\\rho_{t+1}G_{t+1:h}+\\overline V_{h-1}(S_{t+1})-\\rho_{t+1}Q_{h-1}(S_{t+1},A_{t+1})\\big)\n",
    "\\\\ &= R_{t+1}+\\gamma\\rho_{t+1}\\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\\big)+\\gamma\\overline V_{h-1}(S_{t+1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "    where, $t<h\\le T$, if $h<T$, then $G_{h:h}=Q_{h-1}(S_h,A_h)$, else $G_{T-1:h}=R_T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm\n",
    "- Use **left nodes** to estimate action-values\n",
    "\n",
    "\n",
    "![example](assets/7.5.off-policy-wto-weight.png)\n",
    "\n",
    "\n",
    "- one-step return is them same as Expected Sarsa for $t<T-1$:\n",
    "$$G_{t:t+1}=R_{t+1}+\\gamma\\sum_a\\pi(a | S_{t+1})Q_t(S_{t+1},a)$$\n",
    "- two-step tree-backup for $t<T-2$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_{t:t+1} &= R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a | S_{t+1})Q_{t+1}(S_{t+1},a)\n",
    "\\\\ & ~~~ +\\gamma\\pi(A_{t+1} | S_{t+1})\\big(R_{t+1:t+2}\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a | S_{t+2})Q_{t+1}(S_{t+2},a)\\big)\n",
    "\\\\ &= R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a | S_{t+1})Q_{t+1}(S_{t+1},a)+\\gamma\\pi(A_{t+1} | S_{t+1})Q_{t+1:t+2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- n-step tree-backup for $t<T-1,n\\ge 2$:\n",
    "$$G_{t:t+1} = R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a | S_{t+1})Q_{t+1}(S_{t+1},a)+\\gamma\\pi(A_{t+1} | S_{t+1})Q_{t+1:t+n}$$\n",
    "\n",
    "- action-value update rule as usual from n-step Sarsa:\n",
    "$$Q_{t+n}(S_t,A_t)=Q_{t+n-1}(S_t,A_t)+\\alpha[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)]$$\n",
    "    for, $0\\le t < T$\n",
    "\n",
    "\n",
    "![n-step Tree Backup](assets/7.5.n-step-tree-backup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A Unifying Algorithm: n-step Q(σ)\n",
    "\n",
    "![n-step-types](assets/7.6.n-step-types.png)\n",
    "\n",
    "- $\\sigma_t\\in[0,1]$ denote the degree of sampling on step $t$\n",
    "    - $\\sigma=0$ for full sampling\n",
    "    - $\\sigma=1$ for pure expection\n",
    "\n",
    "- Rewrite the n-step back-up tree as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "G_{t:h} &= R_{t+1}+\\gamma\\sum_{a\\neq A_{t+1}}\\pi(a | S_{t+1})Q_{h-1}(S_{t+1},a)+\\gamma\\pi(A_{t+1} | S_{t+1})G_{t+1:h}\n",
    "\\\\ &= R_{t+1}+\\gamma\\overline V_{h-1}(S_{t+1})-\\gamma\\pi(A_{t+1} | S_{t+1})Q_{h-1}(S_{t+1},A_{t+1})+\\gamma\\pi(A_{t+1} | S_{t+1})G_{t+1:h}\n",
    "\\\\ &= R_{t+1}+\\gamma\\pi(A_{t+1} | S_{t+1})\\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\\big)+\\gamma\\overline V_{h-1}(S_{t+1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "- n-step $Q(\\sigma)$:\n",
    "$$G_{t:h}=R_{t+1}+\\gamma\\big(\\sigma_{t+1}\\rho_{t+1}+(1-\\sigma_{t+1})\\pi(A_{t+1} | S_{t+1})\\big)\\big(G_{t+1:h}-Q_{h-1}(S_{t+1},A_{t+1})\\big)+\\gamma\\overline V_{h-1}(S_{t+1})$$\n",
    "    \n",
    "    where, $t<h\\le T$\n",
    "    - if $h<T$, then $G_{h:h}=Q_{h-1}(S_h,A_h)$\n",
    "    - if $h=T$, then $G_{T-1:T}=R_T$\n",
    "\n",
    "\n",
    "![n-step-q](assets/7.6.n-step-q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
