{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: n-step Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. n-step TD Prediction\n",
    "- Generalize one-step TD(0) method\n",
    "- Temporal difference extends over n-steps\n",
    "\n",
    "![n-step methods](assets/7.1.n-step.png)\n",
    "\n",
    "- Want to update estimated value $v_\\pi(S_t)$ of state $S_t$ from:\n",
    "$$S_t,R_{t+1},S_{t+1},R_{t+1},...,R_T,S_T$$\n",
    "\n",
    "    - for *MC*, target is complete return\n",
    "    $$G_t = R_{t+1}+\\gamma R_{t+3}+\\gamma^2R_{t+3}+...+\\gamma^{T-t-1}R_T$$\n",
    "    - for *TD*, one-step method\n",
    "    $$G_{t:t+1} = R_{t+1}+\\gamma V_t(S_{t+1})$$\n",
    "    - for *two-step TD*, one-step method\n",
    "    $$G_{t:t+2} = R_{t+1}+\\gamma R_{t+2}+\\gamma^2V_{t+1}(S_{t+2})$$\n",
    "    - for *n-step TD*, one-step method with $n\\ge 1, 0\\le t<T-n$\n",
    "    $$\n",
    "    \\begin{cases}\n",
    "    G_{t:t+n} &= R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^nV_{t+n-1}(S_{t+n})\n",
    "    \\\\G_{t:t+n} &= G_t ~~~,\\text{if } t+n\\ge T\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "\n",
    "- Wait for $R_{t+n}, V_{t+n-1}$, until time $t+n$, then update estimate values:\n",
    "$$V_{t+n}(S_t) = V_{t+n-1}(S_t)+\\alpha\\big[G_{t:t+n}-V_{t+n-1}(S_t)\\big] ~~~, 0\\le t<T$$\n",
    "    - all other states remain unchanged: $V_{t+n}(s)=V_{t+n-1}(s), \\forall s\\neq S_t$\n",
    "\n",
    "![n-step methods](assets/7.1.n-step-td.png)\n",
    "\n",
    "\n",
    "- **Error Reduction Property** of n-step returns:\n",
    "$$\\max_s\\big| E_\\pi[G_{t:t+n} | S_t=s]-v_\\pi(s)\\big| \\le \\gamma^n\\max_s\\big| V_{t+n-1}(s)-v_\\pi(s)\\big|, \\forall n\\ge 1$$\n",
    "- Can show formally that n-step TD methods converge to the correct predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. n-step Sarsa\n",
    "- Switch states for actions (state-action pairs) and then use an ε-greedy policy\n",
    "\n",
    "![n-step SARSA](assets/7.2.n-step-sarsa.png)\n",
    "\n",
    "- n-step returns for action-value:\n",
    "$$G_{t:t+n}=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^nQ_{t+n-1}(S_{t+n},A_{t+n})~~~, n\\ge 1, 0\\le t<T-n$$\n",
    "    with $G_{t:t+n}=G_t \\text{ if }t+n\\ge T$\n",
    "\n",
    "\n",
    "- **n-step Sarsa**:\n",
    "    $$Q_{t+n}(S_t,A_t)=Q_{t+n-1}(S_t,A_t)+\\alpha\\big[G_{t:t+n}-Q_{t+n-1}(S_t,A_t)\\big]~~~,0\\le t<T$$\n",
    " \n",
    " \n",
    "- **n-step Expected Sarsa**:\n",
    "$$G_{t:t+n}=R_{t+1}+\\gamma R_{t+2}+...+\\gamma^{n-1}R_{t+n}+\\gamma^n\\overline V_{t+n-1}(S_{t+n},A_{t+n})~~~, t+n<T$$\n",
    "    - where, *expected approximate value* of state $s$:\n",
    "    $$\\overline V_t(s)=\\sum_a\\pi(a | s)Q_t(s,a) ~~~, \\forall s\\in\\mathcal S$$\n",
    "    - if $s$ is terminal, then $\\overline V(s)=0$\n",
    "    \n",
    "![n-step SARSA Pseudocode](assets/7.2.n-step-sarsa-pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. n-step Off-policy Learning\n",
    "- Use relative probability of just n actions:\n",
    "$$\\rho_{t:h}=\\prod_{k=t}^{\\min(h,T-1)}\\frac{\\pi(A_k | S_k)}{b(A_k | S_k)}$$\n",
    "\n",
    "- n-step TD:\n",
    "$$V_{t+n}(S_t)=V_{t+n-1}(S_t)+\\alpha\\color{blue}{\\rho_{t:t+n-1}}\\big[G_{t:t+n}-V_{t+n-1}(S_t)\\big]~~~,0\\le t<T$$\n",
    "\n",
    "- n-step Sarsa:\n",
    "$$Q_{t+n}(S_t,A_t)=V_{t+n-1}(S_t,A_t)+\\alpha\\color{blue}{\\rho_{t+1:t+n}}\\big[G_{t:t+n}-Q_{t+n-1}(S_t,A_1)\\big]~~~,0\\le t<T$$\n",
    "\n",
    "- n-step Expected Sarsa:\n",
    "$$Q_{t+n}(S_t,A_t)=V_{t+n-1}(S_t,A_t)+\\alpha\\color{blue}{\\rho_{t+1:t+n-1}}\\big[G_{t:t+n}-Q_{t+n-1}(S_t,A_1)\\big]~~~,0\\le t<T$$\n",
    "\n",
    "![n-step Off-policy](assets/7.3.n-step-off-policy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Per-decision Methods with Control Variates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Off-policy Learning Without Importance Sampling: The n-step Tree Backup Algorithm\n",
    "\n",
    "![n-step Tree Backup](assets/7.5.n-step-tree-backup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. A Unifying Algorithm: n-step Q(σ)\n",
    "\n",
    "![n-step-types](assets/7.6.n-step-types.png)\n",
    "\n",
    "![n-step-q](assets/7.6.n-step-q.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
