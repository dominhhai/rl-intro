{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Finite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exercise 3.6\n",
    "Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for  1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task?\n",
    "\n",
    "### Solution\n",
    "- Return at each time:\n",
    "$$G_t=-\\gamma^T ~~~, \\forall t=\\overline{1, T}$$\n",
    "    where, $T$ is the number of time-steps of episodic\n",
    "    \n",
    "- Differ from constinuing formulation is, for consitnuting, the return must be the sum all off fail steps:\n",
    "$$G_t=-\\sum_{i\\in\\mathcal T}\\gamma^i$$\n",
    "    where, $\\mathcal T$ is the set of fail steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exercise 3.7\n",
    "Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve?\n",
    "\n",
    "$$\\tag{3.7} G_t=R_{t+1}+R_{t+2}+...+R_T$$\n",
    "\n",
    "### Solution\n",
    "- Return at each step is:\n",
    "$$G_t=R_T=1$$\n",
    "\n",
    "- No way to improve returns of episode, because it depends only on the last reward\n",
    "- Need to decrease the reward by time steps, such as:\n",
    "$$G_t=R_{t+1}+\\frac{1}{2}R_{t+2}+...+\\frac{1}{T-t}R_T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise 3.8\n",
    "Suppose $\\gamma=0.5$ and the following sequence of rewards is received $R_1=1, R_2=2,R_3=6,R_4=3$,and $R_5=2$, with $T=5$. What are $G_0,G_1,...,G_5$? Hint: Work backwards.\n",
    "\n",
    "### Solution\n",
    "- We have:\n",
    "$$G_t=R_{t+1}+\\gamma G_{t+1}$$\n",
    "\n",
    "- Terminate state when $T=5$, has return:\n",
    "$$G_5=0$$\n",
    "\n",
    "- Step 4:\n",
    "$$G_4=R_5+\\gamma*G_5=2+0.5*0=2$$\n",
    "\n",
    "- Step 3:\n",
    "$$G_3=R_4+\\gamma*G_4=3+0.5*2=4$$\n",
    "\n",
    "- Step 2:\n",
    "$$G_2=R_3+\\gamma*G_3=6+0.5*4=8$$\n",
    "\n",
    "- Step 1:\n",
    "$$G_1=R_2+\\gamma*G_2=2+0.5*8=6$$\n",
    "\n",
    "- Step 0:\n",
    "$$G_0=R_1+\\gamma*G_1=1+0.5*6=4$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
